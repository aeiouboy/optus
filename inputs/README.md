# Input Folder Structure for Web Scraping

This directory contains organized URL collections for different types of websites and scraping purposes.

## ğŸ“ Folder Structure

```
inputs/
â”œâ”€â”€ websites/           # General website URLs
â”œâ”€â”€ ecommerce/          # E-commerce product pages
â”œâ”€â”€ social_media/       # Social media profiles and posts
â”œâ”€â”€ news/              # News articles and media sites
â”œâ”€â”€ blogs/             # Blog posts and articles
â”œâ”€â”€ documentation/     # Technical documentation
â”œâ”€â”€ api_docs/          # API documentation sites
â”œâ”€â”€ images/            # Image galleries and media
â””â”€â”€ specialized/       # Specialized scraping targets
```

## ğŸš€ Usage Examples

### Single Category Scraping
```bash
# Scrape all e-commerce URLs
./adws/adw_crawl4ai_scraper.py --input-folder inputs/ecommerce

# Scrape news websites
./adws/adw_crawl4ai_scraper.py --input-folder inputs/news

# Scrape social media
./adws/adw_crawl4ai_scraper.py --input-folder inputs/social_media
```

### Multiple Categories
```bash
# Scrape multiple categories
./adws/adw_crawl4ai_scraper.py --input-folder inputs/ --filter "ecommerce,social_media"
```

## ğŸ“ File Formats Supported

- **`.txt`** - Simple text files (one URL per line)
- **`.urls`** - URL collection files
- **`.list`** - Organized lists
- **`.csv`** - CSV files with URLs in first column

## ğŸ¯ Best Practices

1. **Organize by purpose** - Group similar websites together
2. **Use descriptive names** - Clear file names indicate content
3. **Add comments** - Use `#` for comments in text files
4. **Keep lists manageable** - 50-100 URLs per file is optimal
5. **Test small batches** - Test with few URLs before full runs

## ğŸ”§ File Naming Conventions

- Use lowercase letters
- Separate words with underscores
- Include website/domain name
- Add date or version if needed

Examples:
- `thaiwatsadu_products.txt`
- `amazon_electronics_2024.txt`
- `news_tech_websites.urls`
"""
Crawl4AI wrapper module for providing consistent interface and error handling.

This module wraps the crawl4ai library to provide:
- Scraping configuration management
- URL validation and preprocessing
- Content extraction methods
- Error handling for network issues and anti-bot measures
- Output formatting functions
"""

import asyncio
import json
import re
import time
import urllib.parse
from typing import List, Dict, Any, Optional, Union, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import logging

try:
    from crawl4ai import AsyncWebCrawler
    from crawl4ai.extraction_strategy import LLMExtractionStrategy, JsonCssExtractionStrategy
    try:
        from crawl4ai.llm_config import LLMConfig
    except ImportError:
        try:
            from crawl4ai.extraction_strategy import LLMConfig
        except ImportError:
            LLMConfig = None
    
    from crawl4ai.chunking_strategy import RegexChunking
    CRAWL4AI_AVAILABLE = True
except ImportError:
    CRAWL4AI_AVAILABLE = False
    AsyncWebCrawler = None
    LLMExtractionStrategy = None
    JsonCssExtractionStrategy = None
    RegexChunking = None
    LLMConfig = None

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ScrapingConfig:
    """Configuration for web scraping operations."""
    max_concurrent: int = 3
    delay_between_requests: float = 1.0
    timeout: int = 30
    user_agent: str = "Mozilla/5.0 (compatible; Crawl4AI/1.0)"
    headless: bool = True
    verbose: bool = False
    retry_attempts: int = 3
    retry_delay: float = 2.0

    # Anti-bot measures
    respect_robots_txt: bool = True
    use_browser: bool = True
    simulate_user: bool = True

    # Content filtering
    min_content_length: int = 100
    max_content_length: int = 1000000  # 1MB

    # Output options
    include_links: bool = True
    include_images: bool = True
    include_metadata: bool = True


@dataclass
class ScrapingResult:
    """Result of a scraping operation."""
    url: str
    success: bool
    content: Optional[str] = None
    markdown: Optional[str] = None
    html: Optional[str] = None
    links: List[str] = None
    images: List[str] = None
    metadata: Dict[str, Any] = None
    error_message: Optional[str] = None
    timestamp: float = 0
    status_code: Optional[int] = None
    extracted_content: Optional[str] = None

    def __post_init__(self):
        if self.links is None:
            self.links = []
        if self.images is None:
            self.images = []
        if self.metadata is None:
            self.metadata = {}
        if self.timestamp == 0:
            self.timestamp = time.time()


class Crawl4AIWrapper:
    """Wrapper class for crawl4ai functionality."""

    def __init__(self, config: ScrapingConfig = None):
        """Initialize the wrapper with configuration.

        Args:
            config: Scraping configuration. If None, uses defaults.
        """
        self.config = config or ScrapingConfig()

        if not CRAWL4AI_AVAILABLE:
            raise ImportError(
                "crawl4ai is not installed. Install it with: "
                "pip install crawl4ai"
            )

        self.crawler = None

    async def __aenter__(self):
        """Async context manager entry."""
        await self.initialize()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        await self.close()

    async def initialize(self):
        """Initialize the crawler instance."""
        try:
            self.crawler = AsyncWebCrawler(
                headless=self.config.headless,
                verbose=self.config.verbose,
                user_agent=self.config.user_agent,
            )
            await self.crawler.start()
            logger.info("Crawl4AI crawler initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize Crawl4AI crawler: {e}")
            raise

    async def close(self):
        """Close the crawler instance."""
        if self.crawler:
            try:
                await self.crawler.close()
                logger.info("Crawl4AI crawler closed successfully")
            except Exception as e:
                logger.error(f"Error closing Crawl4AI crawler: {e}")

    def validate_url(self, url: str) -> Tuple[bool, Optional[str]]:
        """Validate and normalize URL.

        Args:
            url: URL to validate

        Returns:
            Tuple of (is_valid, normalized_url_or_error)
        """
        if not url or not isinstance(url, str):
            return False, "URL must be a non-empty string"

        url = url.strip()
        if not url:
            return False, "URL cannot be empty"

        # Add scheme if missing
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url

        try:
            parsed = urllib.parse.urlparse(url)
            if not parsed.netloc:
                return False, "Invalid URL format"
            return True, url
        except Exception as e:
            return False, f"URL parsing error: {e}"

    async def scrape_url(
        self,
        url: str,
        extraction_strategy: Optional[Any] = None,
        wait_for: Optional[str] = None,
        css_selector: Optional[str] = None
    ) -> ScrapingResult:
        """Scrape a single URL.

        Args:
            url: URL to scrape
            extraction_strategy: Optional extraction strategy
            wait_for: Optional JavaScript condition to wait for
            css_selector: Optional CSS selector to wait for

        Returns:
            ScrapingResult with scraped data
        """
        is_valid, normalized_url_or_error = self.validate_url(url)
        if not is_valid:
            return ScrapingResult(
                url=url,
                success=False,
                error_message=normalized_url_or_error
            )

        url = normalized_url_or_error

        if not self.crawler:
            await self.initialize()

        result = ScrapingResult(url=url, success=False)

        for attempt in range(self.config.retry_attempts):
            try:
                # Add delay between requests (except first attempt)
                if attempt > 0:
                    await asyncio.sleep(self.config.retry_delay * (attempt + 1))

                logger.info(f"Scraping URL: {url} (attempt {attempt + 1})")

                # Perform the crawl
                crawl_result = await self.crawler.arun(
                    url=url,
                    word_count_threshold=self.config.min_content_length,
                    extraction_strategy=extraction_strategy,
                    bypass_cache=False,
                    js_code="""
                    // Wait for dynamic content to load
                    await new Promise(resolve => setTimeout(resolve, 2000));
                    """ if self.config.use_browser else None,
                    wait_for=wait_for or ("""
                    () => document.readyState === 'complete' &&
                    document.body && document.body.innerText.length > 100
                    """ if self.config.use_browser else None),
                    css_selector=css_selector or ("""
                    body
                    """ if self.config.use_browser else None),
                    simulate_user=self.config.simulate_user,
                    override_navigator=True,
                )

                if crawl_result.success:
                    result.success = True
                    result.content = crawl_result.cleaned_html or crawl_result.html
                    result.markdown = str(crawl_result.markdown) if crawl_result.markdown else None
                    result.html = crawl_result.html
                    result.status_code = getattr(crawl_result, 'status_code', 200)
                    result.extracted_content = getattr(crawl_result, 'extracted_content', None)

                    # Extract links
                    if self.config.include_links and crawl_result.links:
                        try:
                            result.links = [link.get('href', '') for link in crawl_result.links
                                          if link and hasattr(link, 'get') and link.get('href')]
                        except (TypeError, AttributeError):
                            result.links = []

                    # Extract images
                    if self.config.include_images and crawl_result.media:
                        try:
                            result.images = [media.get('src', '') for media in crawl_result.media
                                          if media and hasattr(media, 'get') and media.get('src') and media.get('type') == 'image']
                        except (TypeError, AttributeError):
                            result.images = []

                    # Extract metadata
                    if self.config.include_metadata:
                        # Enhanced metadata for new structured output
                        domain = self.get_domain_from_url(url)
                        content_type = self.detect_content_type(url, result.content, result.metadata)

                        result.metadata = {
                            'title': getattr(crawl_result, 'title', ''),
                            'description': getattr(crawl_result, 'description', ''),
                            'language': getattr(crawl_result, 'language', ''),
                            'status_code': result.status_code,
                            'url': url,
                            'word_count': len(result.content.split()) if result.content else 0,
                            # New fields for structured output
                            'domain': domain,
                            'content_type': content_type,
                            'scraped_at': result.timestamp,
                            'extraction_method': 'crawl4ai',
                            'links_count': len(result.links) if result.links else 0,
                            'images_count': len(result.images) if result.images else 0,
                        }

                    logger.info(f"Successfully scraped {url}")
                    break
                else:
                    error_msg = getattr(crawl_result, 'error_message', 'Unknown error')
                    result.error_message = f"Crawl failed: {error_msg}"
                    logger.warning(f"Failed to scrape {url}: {error_msg}")

            except Exception as e:
                result.error_message = f"Scraping error: {str(e)}"
                logger.error(f"Error scraping {url} (attempt {attempt + 1}): {e}")

                if attempt == self.config.retry_attempts - 1:
                    # Final attempt failed
                    break

        return result

    async def scrape_urls(
        self,
        urls: List[str],
        extraction_strategy: Optional[Any] = None
    ) -> List[ScrapingResult]:
        """Scrape multiple URLs with concurrency control.

        Args:
            urls: List of URLs to scrape
            extraction_strategy: Optional extraction strategy

        Returns:
            List of ScrapingResult objects
        """
        if not urls:
            return []

        results = []

        # Process URLs in batches to control concurrency
        batch_size = self.config.max_concurrent

        for i in range(0, len(urls), batch_size):
            batch = urls[i:i + batch_size]

            # Create semaphore to limit concurrent requests
            semaphore = asyncio.Semaphore(batch_size)

            async def scrape_with_semaphore(url: str) -> ScrapingResult:
                async with semaphore:
                    result = await self.scrape_url(url, extraction_strategy)
                    # Add delay between requests
                    if self.config.delay_between_requests > 0:
                        await asyncio.sleep(self.config.delay_between_requests)
                    return result

            # Process batch concurrently
            batch_tasks = [scrape_with_semaphore(url) for url in batch]
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

            # Handle exceptions in results
            for url, batch_result in zip(batch, batch_results):
                if isinstance(batch_result, Exception):
                    results.append(ScrapingResult(
                        url=url,
                        success=False,
                        error_message=f"Batch processing error: {str(batch_result)}"
                    ))
                else:
                    results.append(batch_result)

            logger.info(f"Completed batch {i//batch_size + 1}/{(len(urls) + batch_size - 1)//batch_size}")

        return results

    def create_json_extraction_strategy(self, schema: Dict[str, Any]) -> Any:
        """Create a JSON CSS extraction strategy.

        Args:
            schema: JSON schema for extraction

        Returns:
            JsonCssExtractionStrategy instance
        """
        if not JsonCssExtractionStrategy:
            raise ImportError("JsonCssExtractionStrategy not available")

        return JsonCssExtractionStrategy(schema, verbose=self.config.verbose)

    def create_llm_extraction_strategy(
        self,
        instruction: str,
        provider: str = "openai",
        api_token: Optional[str] = None,
        **kwargs
    ) -> Any:
        """Create an LLM extraction strategy.

        Args:
            instruction: Extraction instruction for the LLM
            provider: LLM provider (openai, huggingface, etc.)
            api_token: API token for the provider
            **kwargs: Additional arguments for the strategy (e.g. base_url, model)

        Returns:
            LLMExtractionStrategy instance
        """
        if not LLMExtractionStrategy:
            raise ImportError("LLMExtractionStrategy not available")

        # Create LLMConfig with provider, token, and extra args
        # We pass kwargs (like base_url, model) to LLMConfig
        llm_config = LLMConfig(provider=provider, api_token=api_token, **kwargs)

        return LLMExtractionStrategy(
            llm_config=llm_config,
            instruction=instruction,
            verbose=self.config.verbose
        )

    def format_results(self, results: List[ScrapingResult], format_type: str = "json") -> str:
        """Format scraping results for output.

        Args:
            results: List of ScrapingResult objects
            format_type: Output format ('json', 'csv', 'markdown')

        Returns:
            Formatted string
        """
        if format_type.lower() == "json":
            return json.dumps([asdict(result) for result in results], indent=2)

        elif format_type.lower() == "csv":
            import csv
            import io

            if not results:
                return ""

            output = io.StringIO()

            # Write header
            fieldnames = ['url', 'success', 'content_length', 'links_count',
                         'images_count', 'status_code', 'error_message']
            writer = csv.DictWriter(output, fieldnames=fieldnames)
            writer.writeheader()

            # Write rows
            for result in results:
                writer.writerow({
                    'url': result.url,
                    'success': result.success,
                    'content_length': len(result.content) if result.content else 0,
                    'links_count': len(result.links),
                    'images_count': len(result.images),
                    'status_code': result.status_code or '',
                    'error_message': result.error_message or '',
                })

            return output.getvalue()

        elif format_type.lower() == "markdown":
            if not results:
                return "# No Results\n"

            output = ["# Scraping Results\n"]
            output.append(f"Total URLs processed: {len(results)}")
            output.append(f"Successful: {sum(1 for r in results if r.success)}")
            output.append(f"Failed: {sum(1 for r in results if not r.success)}\n")

            for result in results:
                output.append(f"## {result.url}")
                output.append(f"**Status:** {'✅ Success' if result.success else '❌ Failed'}")

                if result.success:
                    if result.metadata.get('title'):
                        output.append(f"**Title:** {result.metadata['title']}")
                    if result.content:
                        output.append(f"**Content Length:** {len(result.content)} characters")
                    if result.links:
                        output.append(f"**Links Found:** {len(result.links)}")
                    if result.images:
                        output.append(f"**Images Found:** {len(result.images)}")
                    if result.markdown:
                        preview = result.markdown[:200] + "..." if len(result.markdown) > 200 else result.markdown
                        output.append(f"**Content Preview:**\n{preview}")
                else:
                    output.append(f"**Error:** {result.error_message}")

                output.append("")  # Empty line between results

            return "\n".join(output)

        else:
            raise ValueError(f"Unsupported format type: {format_type}")

    def get_domain_from_url(self, url: str) -> str:
        """Extract domain name from URL.

        Args:
            url: URL to extract domain from

        Returns:
            Domain name as string
        """
        try:
            parsed = urllib.parse.urlparse(url)
            domain = parsed.netloc.lower()
            # Remove www. prefix if present
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        except Exception as e:
            logger.warning(f"Failed to extract domain from {url}: {e}")
            return "unknown_domain"

    def detect_content_type(self, url: str, content: str = None, metadata: Dict = None) -> str:
        """Detect content type based on URL, content, and metadata.

        Args:
            url: URL that was scraped
            content: Raw content (optional)
            metadata: Additional metadata (optional)

        Returns:
            Detected content type as string
        """
        if metadata is None:
            metadata = {}

        url_lower = url.lower()

        # Check URL patterns first
        if any(pattern in url_lower for pattern in ['/product', '/item', '/shop', '/buy', '/cart']):
            return 'products'
        elif any(pattern in url_lower for pattern in ['/article', '/blog', '/news', '/post', '/story']):
            return 'articles'
        elif any(pattern in url_lower for pattern in ['/doc', '/documentation', '/guide', '/help', '/wiki']):
            return 'documentation'
        elif any(pattern in url_lower for pattern in ['/api', '/endpoint', '/service']):
            return 'api'
        elif any(pattern in url_lower for pattern in ['/forum', '/discussion', '/thread', '/comment']):
            return 'forum'
        elif any(pattern in url_lower for pattern in ['/video', '/watch', '/play', '/stream']):
            return 'video'

        # Check metadata if available
        title = str(metadata.get('title', '')).lower()
        description = str(metadata.get('description', '')).lower()

        if any(keyword in title + description for keyword in ['product', 'buy', 'price', 'shop', 'cart', 'purchase']):
            return 'products'
        elif any(keyword in title + description for keyword in ['article', 'blog', 'news', 'post', 'story', 'published']):
            return 'articles'
        elif any(keyword in title + description for keyword in ['documentation', 'guide', 'help', 'manual', 'tutorial']):
            return 'documentation'

        # Analyze content if available
        if content:
            content_lower = content.lower()
            if any(keyword in content_lower for keyword in ['price', 'cart', 'checkout', 'buy now', 'add to cart']):
                return 'products'
            elif any(keyword in content_lower for keyword in ['article', 'published', 'author', 'posted on']):
                return 'articles'
            elif any(keyword in content_lower for keyword in ['documentation', 'guide', 'tutorial', 'step by step']):
                return 'documentation'

        # Additional heuristics based on common patterns
        if any(site in domain for site in ['amazon', 'ebay', 'shopify', 'woocommerce'] for domain in [self.get_domain_from_url(url)]):
            return 'products'
        elif any(site in domain for site in ['wikipedia', 'wiki', 'docs'] for domain in [self.get_domain_from_url(url)]):
            return 'documentation'

        # Default fallback
        return 'general'

    def enhance_result_for_organization(self, result: ScrapingResult) -> ScrapingResult:
        """Enhance a scraping result with organization metadata.

        Args:
            result: ScrapingResult to enhance

        Returns:
            Enhanced ScrapingResult
        """
        if not result.metadata:
            result.metadata = {}

        # Add organization-specific metadata
        result.metadata.update({
            'domain': result.metadata.get('domain', self.get_domain_from_url(result.url)),
            'content_type': result.metadata.get('content_type', self.detect_content_type(result.url, result.content, result.metadata)),
            'organization_timestamp': result.timestamp,
            'result_id': f"{result.url}_{int(result.timestamp)}",
            'has_content': bool(result.content and len(result.content.strip()) > 100),
            'has_links': bool(result.links and len(result.links) > 0),
            'has_images': bool(result.images and len(result.images) > 0),
        })

        return result


# Convenience functions for common use cases
async def scrape_single_url(
    url: str,
    config: ScrapingConfig = None,
    extraction_strategy: Optional[Any] = None
) -> ScrapingResult:
    """Scrape a single URL with default configuration.

    Args:
        url: URL to scrape
        config: Optional scraping configuration
        extraction_strategy: Optional extraction strategy

    Returns:
        ScrapingResult object
    """
    wrapper = Crawl4AIWrapper(config or ScrapingConfig())
    async with wrapper:
        return await wrapper.scrape_url(url, extraction_strategy)


async def scrape_multiple_urls(
    urls: List[str],
    config: ScrapingConfig = None,
    extraction_strategy: Optional[Any] = None
) -> List[ScrapingResult]:
    """Scrape multiple URLs with default configuration.

    Args:
        urls: List of URLs to scrape
        config: Optional scraping configuration
        extraction_strategy: Optional extraction strategy

    Returns:
        List of ScrapingResult objects
    """
    wrapper = Crawl4AIWrapper(config or ScrapingConfig())
    async with wrapper:
        return await wrapper.scrape_urls(urls, extraction_strategy)


def create_simple_config(**kwargs) -> ScrapingConfig:
    """Create a ScrapingConfig with common customizations.

    Args:
        **kwargs: Configuration parameters to override

    Returns:
        ScrapingConfig instance
    """
    defaults = {
        'max_concurrent': 3,
        'delay_between_requests': 1.0,
        'timeout': 30,
        'verbose': False,
    }
    defaults.update(kwargs)
    return ScrapingConfig(**defaults)
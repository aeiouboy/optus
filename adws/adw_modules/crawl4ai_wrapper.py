"""
Crawl4AI wrapper module for providing consistent interface and error handling.

This module wraps the crawl4ai library to provide:
- Scraping configuration management
- URL validation and preprocessing
- Content extraction methods
- Error handling for network issues and anti-bot measures
- Output formatting functions
"""

import asyncio
import json
import re
import time
import urllib.parse
from typing import List, Dict, Any, Optional, Union, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import logging

try:
    from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
    from crawl4ai.extraction_strategy import LLMExtractionStrategy, JsonCssExtractionStrategy
    try:
        from crawl4ai.llm_config import LLMConfig
    except ImportError:
        try:
            from crawl4ai.extraction_strategy import LLMConfig
        except ImportError:
            LLMConfig = None

    from crawl4ai.chunking_strategy import RegexChunking
    CRAWL4AI_AVAILABLE = True
except ImportError:
    CRAWL4AI_AVAILABLE = False
    AsyncWebCrawler = None
    BrowserConfig = None
    CrawlerRunConfig = None
    CacheMode = None
    LLMExtractionStrategy = None
    JsonCssExtractionStrategy = None
    RegexChunking = None
    LLMConfig = None

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ScrapingConfig:
    """Configuration for web scraping operations."""
    max_concurrent: int = 2  # Reduced from 3 to avoid network errors
    delay_between_requests: float = 1.5  # Increased from 1.0 to be polite
    timeout: int = 30
    user_agent: str = "Mozilla/5.0 (compatible; Crawl4AI/1.0)"
    headless: bool = True
    verbose: bool = False
    retry_attempts: int = 3
    retry_delay: float = 2.0

    # Anti-bot measures
    respect_robots_txt: bool = True
    use_browser: bool = True
    simulate_user: bool = True

    # Content filtering
    min_content_length: int = 100
    max_content_length: int = 1000000  # 1MB

    # Output options
    include_links: bool = True
    include_images: bool = True
    include_metadata: bool = True

    # Performance optimization options (optimized for price updates)
    use_cache: bool = True  # Enable cache by default for speed (use --no-cache for full refresh)
    stream_results: bool = True  # Enables streaming for faster processing


@dataclass
class ScrapingResult:
    """Result of a scraping operation."""
    url: str
    success: bool
    content: Optional[str] = None
    markdown: Optional[str] = None
    html: Optional[str] = None
    links: List[str] = None
    images: List[str] = None
    metadata: Dict[str, Any] = None
    error_message: Optional[str] = None
    timestamp: float = 0
    status_code: Optional[int] = None
    extracted_content: Optional[str] = None

    def __post_init__(self):
        if self.links is None:
            self.links = []
        if self.images is None:
            self.images = []
        if self.metadata is None:
            self.metadata = {}
        if self.timestamp == 0:
            self.timestamp = time.time()


class Crawl4AIWrapper:
    """Wrapper class for crawl4ai functionality."""

    def __init__(self, config: ScrapingConfig = None):
        """Initialize the wrapper with configuration.

        Args:
            config: Scraping configuration. If None, uses defaults.
        """
        self.config = config or ScrapingConfig()

        if not CRAWL4AI_AVAILABLE:
            raise ImportError(
                "crawl4ai is not installed. Install it with: "
                "pip install crawl4ai"
            )

        self.crawler = None

    async def __aenter__(self):
        """Async context manager entry."""
        await self.initialize()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        await self.close()

    async def initialize(self):
        """Initialize the crawler instance with retry logic."""
        max_init_attempts = 3
        for attempt in range(max_init_attempts):
            try:
                browser_config = BrowserConfig(
                    headless=self.config.headless,
                    verbose=self.config.verbose,
                    user_agent=self.config.user_agent,
                )
                self.crawler = AsyncWebCrawler(config=browser_config)
                await self.crawler.start()
                logger.info("Crawl4AI crawler initialized successfully")
                return
            except Exception as e:
                logger.warning(f"Browser launch failed (attempt {attempt + 1}/{max_init_attempts}): {e}")
                if attempt < max_init_attempts - 1:
                    # Clean up failed instance
                    if self.crawler:
                        try:
                            await self.crawler.close()
                        except:
                            pass
                        self.crawler = None
                    # Wait before retry
                    await asyncio.sleep(2 * (attempt + 1))
                else:
                    logger.error(f"Failed to initialize Crawl4AI crawler after {max_init_attempts} attempts")
                    raise

    async def close(self):
        """Close the crawler instance."""
        if self.crawler:
            try:
                await self.crawler.close()
                logger.info("Crawl4AI crawler closed successfully")
            except Exception as e:
                logger.error(f"Error closing Crawl4AI crawler: {e}")

    async def ensure_crawler_ready(self):
        """Ensure crawler is initialized and healthy, reinitialize if needed."""
        if not self.crawler:
            await self.initialize()
            return

        # Check if crawler is still alive by checking browser state
        try:
            # If crawler exists but browser is closed, we need to reinitialize
            if hasattr(self.crawler, 'browser') and self.crawler.browser is not None:
                # Browser exists, check if it's still connected
                if hasattr(self.crawler.browser, 'is_connected'):
                    if not self.crawler.browser.is_connected():
                        logger.warning("Browser disconnected, reinitializing crawler...")
                        await self.close()
                        self.crawler = None
                        await self.initialize()
        except Exception as e:
            # If health check fails, reinitialize
            logger.warning(f"Crawler health check failed: {e}, reinitializing...")
            try:
                await self.close()
            except:
                pass
            self.crawler = None
            await self.initialize()

    def create_run_config(self, wait_for: Optional[str] = None) -> Optional[Any]:
        """Create a CrawlerRunConfig with optimized settings.

        Args:
            wait_for: Optional JavaScript condition to wait for (caller override)

        Returns:
            CrawlerRunConfig instance or None if not available
        """
        if CrawlerRunConfig is None or CacheMode is None:
            logger.debug("CrawlerRunConfig or CacheMode not available, using legacy mode")
            return None

        try:
            return CrawlerRunConfig(
                cache_mode=CacheMode.ENABLED if self.config.use_cache else CacheMode.BYPASS,
                stream=self.config.stream_results,
                only_text=False,  # Keep images - preserve image extraction capability
                wait_for=wait_for
            )
        except Exception as e:
            logger.warning(f"Failed to create CrawlerRunConfig: {e}, falling back to legacy mode")
            return None

    def validate_url(self, url: str) -> Tuple[bool, Optional[str]]:
        """Validate and normalize URL.

        Args:
            url: URL to validate

        Returns:
            Tuple of (is_valid, normalized_url_or_error)
        """
        if not url or not isinstance(url, str):
            return False, "URL must be a non-empty string"

        url = url.strip()
        if not url:
            return False, "URL cannot be empty"

        # Add scheme if missing
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url

        try:
            parsed = urllib.parse.urlparse(url)
            if not parsed.netloc:
                return False, "Invalid URL format"
            return True, url
        except Exception as e:
            return False, f"URL parsing error: {e}"

    def is_ecommerce_url(self, url: str) -> bool:
        """Check if a URL belongs to supported e-commerce retailers.

        Args:
            url: URL to check

        Returns:
            True if the URL belongs to a supported e-commerce retailer, False otherwise
        """
        try:
            # Extract domain from URL
            parsed = urllib.parse.urlparse(url.lower())
            domain = parsed.netloc.lower()

            # Remove www. prefix if present
            if domain.startswith('www.'):
                domain = domain[4:]

            # List of supported e-commerce retailers
            supported_retailers = {
                'thaiwatsadu.com',
                'homepro.co.th',
                'dohome.co.th',
                'boonthavorn.com',
                'globalhouse.co.th',
                'megahome.co.th'
            }

            return domain in supported_retailers
        except Exception as e:
            logger.warning(f"Failed to check e-commerce URL {url}: {e}")
            return False

    async def scrape_url(
        self,
        url: str,
        extraction_strategy: Optional[Any] = None,
        wait_for: Optional[str] = None,
        css_selector: Optional[str] = None
    ) -> ScrapingResult:
        """Scrape a single URL.

        Args:
            url: URL to scrape
            extraction_strategy: Optional extraction strategy
            wait_for: Optional JavaScript condition to wait for
            css_selector: Optional CSS selector to wait for

        Returns:
            ScrapingResult with scraped data
        """
        is_valid, normalized_url_or_error = self.validate_url(url)
        if not is_valid:
            return ScrapingResult(
                url=url,
                success=False,
                error_message=normalized_url_or_error
            )

        url = normalized_url_or_error

        # Ensure crawler is ready before scraping
        await self.ensure_crawler_ready()

        result = ScrapingResult(url=url, success=False)

        # Create run config for optimized crawling
        default_wait_for = wait_for or ("""
            () => document.readyState === 'complete' &&
            document.body && document.body.innerText.length > 100
            """ if self.config.use_browser else None)
        run_config = self.create_run_config(wait_for=default_wait_for)

        for attempt in range(self.config.retry_attempts):
            try:
                # Add delay between requests (except first attempt)
                if attempt > 0:
                    await asyncio.sleep(self.config.retry_delay * (attempt + 1))

                logger.info(f"Scraping URL: {url} (attempt {attempt + 1})")

                # Ensure crawler is still healthy before each attempt
                await self.ensure_crawler_ready()

                # Perform the crawl with CrawlerRunConfig if available
                if run_config is not None:
                    # Use new optimized CrawlerRunConfig
                    crawl_result = await self.crawler.arun(
                        url=url,
                        config=run_config,
                        word_count_threshold=self.config.min_content_length,
                        extraction_strategy=extraction_strategy,
                        js_code="""
                        // Wait for dynamic content to load
                        await new Promise(resolve => setTimeout(resolve, 2000));
                        """ if self.config.use_browser else None,
                        css_selector=css_selector or ("""
                        body
                        """ if self.config.use_browser else None),
                        simulate_user=self.config.simulate_user,
                        override_navigator=True,
                    )
                else:
                    # Fallback to legacy mode
                    crawl_result = await self.crawler.arun(
                        url=url,
                        word_count_threshold=self.config.min_content_length,
                        extraction_strategy=extraction_strategy,
                        bypass_cache=False,
                        js_code="""
                        // Wait for dynamic content to load
                        await new Promise(resolve => setTimeout(resolve, 2000));
                        """ if self.config.use_browser else None,
                        wait_for=default_wait_for,
                        css_selector=css_selector or ("""
                        body
                        """ if self.config.use_browser else None),
                        simulate_user=self.config.simulate_user,
                        override_navigator=True,
                    )

                if crawl_result.success:
                    result.success = True
                    result.content = crawl_result.cleaned_html or crawl_result.html
                    result.markdown = str(crawl_result.markdown) if crawl_result.markdown else None
                    result.html = crawl_result.html
                    result.status_code = getattr(crawl_result, 'status_code', 200)
                    result.extracted_content = getattr(crawl_result, 'extracted_content', None)

                    # Extract links
                    if self.config.include_links and crawl_result.links:
                        try:
                            result.links = [link.get('href', '') for link in crawl_result.links
                                          if link and hasattr(link, 'get') and link.get('href')]
                        except (TypeError, AttributeError):
                            result.links = []

                    # Extract images
                    if self.config.include_images and crawl_result.media:
                        try:
                            result.images = [media.get('src', '') for media in crawl_result.media
                                          if media and hasattr(media, 'get') and media.get('src') and media.get('type') == 'image']
                        except (TypeError, AttributeError):
                            result.images = []

                    # Extract metadata
                    if self.config.include_metadata:
                        # Enhanced metadata for new structured output
                        domain = self.get_domain_from_url(url)
                        content_type = self.detect_content_type(url, result.content, result.metadata)

                        result.metadata = {
                            'title': getattr(crawl_result, 'title', ''),
                            'description': getattr(crawl_result, 'description', ''),
                            'language': getattr(crawl_result, 'language', ''),
                            'status_code': result.status_code,
                            'url': url,
                            'word_count': len(result.content.split()) if result.content else 0,
                            # New fields for structured output
                            'domain': domain,
                            'content_type': content_type,
                            'scraped_at': result.timestamp,
                            'extraction_method': 'crawl4ai',
                            'links_count': len(result.links) if result.links else 0,
                            'images_count': len(result.images) if result.images else 0,
                        }

                    logger.info(f"Successfully scraped {url}")
                    break
                else:
                    error_msg = getattr(crawl_result, 'error_message', 'Unknown error')
                    result.error_message = f"Crawl failed: {error_msg}"
                    logger.warning(f"Failed to scrape {url}: {error_msg}")

            except Exception as e:
                error_str = str(e)
                result.error_message = f"Scraping error: {error_str}"
                logger.error(f"Error scraping {url} (attempt {attempt + 1}): {e}")

                # Check if it's a browser-related error that requires reinitialization
                browser_error_keywords = [
                    "Target page, context or browser has been closed",
                    "Browser has been closed",
                    "Connection closed",
                    "Protocol error",
                    "Browser is not connected"
                ]

                if any(keyword in error_str for keyword in browser_error_keywords):
                    logger.warning(f"Detected browser crash/closure, reinitializing for next attempt...")
                    try:
                        await self.close()
                    except:
                        pass
                    self.crawler = None
                    # Will be reinitialized on next attempt via ensure_crawler_ready()

                if attempt == self.config.retry_attempts - 1:
                    # Final attempt failed
                    break

        return result

    async def scrape_urls(
        self,
        urls: List[str],
        extraction_strategy: Optional[Any] = None
    ) -> List[ScrapingResult]:
        """Scrape multiple URLs with concurrency control.

        Uses arun_many() for optimized batch processing when available,
        with streaming support for immediate result processing.

        Args:
            urls: List of URLs to scrape
            extraction_strategy: Optional extraction strategy

        Returns:
            List of ScrapingResult objects
        """
        if not urls:
            return []

        if not self.crawler:
            await self.initialize()

        # Validate URLs first
        validated_urls = []
        invalid_results = []
        for url in urls:
            is_valid, normalized_url_or_error = self.validate_url(url)
            if is_valid:
                validated_urls.append(normalized_url_or_error)
            else:
                invalid_results.append(ScrapingResult(
                    url=url,
                    success=False,
                    error_message=normalized_url_or_error
                ))

        if not validated_urls:
            return invalid_results

        results = []

        # Try to use arun_many for optimized batch processing
        run_config = self.create_run_config()
        use_arun_many = (
            run_config is not None and
            hasattr(self.crawler, 'arun_many') and
            self.config.stream_results
        )

        if use_arun_many:
            try:
                logger.info(f"Using arun_many for batch processing {len(validated_urls)} URLs")

                # Use arun_many for better concurrency handling
                crawl_results = await self.crawler.arun_many(
                    urls=validated_urls,
                    config=run_config,
                    word_count_threshold=self.config.min_content_length,
                    extraction_strategy=extraction_strategy,
                )

                # Process results as they arrive (streaming)
                url_index = 0
                async for crawl_result in crawl_results:
                    url = validated_urls[url_index] if url_index < len(validated_urls) else "unknown"
                    url_index += 1

                    result = self._process_crawl_result(crawl_result, url)
                    results.append(result)

                    # Add delay between processing (for rate limiting)
                    if self.config.delay_between_requests > 0:
                        await asyncio.sleep(self.config.delay_between_requests)

                    logger.info(f"Processed {url_index}/{len(validated_urls)} URLs")

                # Combine with invalid results
                return invalid_results + results

            except Exception as e:
                logger.warning(f"arun_many failed: {e}, falling back to individual scraping")
                # Fall through to legacy batch processing

        # Fallback: Process URLs in batches using individual arun() calls
        logger.info(f"Using legacy batch processing for {len(validated_urls)} URLs")
        batch_size = self.config.max_concurrent

        for i in range(0, len(validated_urls), batch_size):
            batch = validated_urls[i:i + batch_size]

            # Create semaphore to limit concurrent requests
            semaphore = asyncio.Semaphore(batch_size)

            async def scrape_with_semaphore(url: str) -> ScrapingResult:
                async with semaphore:
                    result = await self.scrape_url(url, extraction_strategy)
                    # Add delay between requests
                    if self.config.delay_between_requests > 0:
                        await asyncio.sleep(self.config.delay_between_requests)
                    return result

            # Process batch concurrently
            batch_tasks = [scrape_with_semaphore(url) for url in batch]
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

            # Handle exceptions in results
            for url, batch_result in zip(batch, batch_results):
                if isinstance(batch_result, Exception):
                    results.append(ScrapingResult(
                        url=url,
                        success=False,
                        error_message=f"Batch processing error: {str(batch_result)}"
                    ))
                else:
                    results.append(batch_result)

            logger.info(f"Completed batch {i//batch_size + 1}/{(len(validated_urls) + batch_size - 1)//batch_size}")

        return invalid_results + results

    def _process_crawl_result(self, crawl_result: Any, url: str) -> ScrapingResult:
        """Process a single crawl result into a ScrapingResult.

        Args:
            crawl_result: Raw crawl result from crawler
            url: URL that was crawled

        Returns:
            ScrapingResult object
        """
        result = ScrapingResult(url=url, success=False)

        try:
            if crawl_result.success:
                result.success = True
                result.content = crawl_result.cleaned_html or crawl_result.html
                result.markdown = str(crawl_result.markdown) if crawl_result.markdown else None
                result.html = crawl_result.html
                result.status_code = getattr(crawl_result, 'status_code', 200)
                result.extracted_content = getattr(crawl_result, 'extracted_content', None)

                # Extract links
                if self.config.include_links and crawl_result.links:
                    try:
                        result.links = [link.get('href', '') for link in crawl_result.links
                                      if link and hasattr(link, 'get') and link.get('href')]
                    except (TypeError, AttributeError):
                        result.links = []

                # Extract images
                if self.config.include_images and crawl_result.media:
                    try:
                        result.images = [media.get('src', '') for media in crawl_result.media
                                      if media and hasattr(media, 'get') and media.get('src') and media.get('type') == 'image']
                    except (TypeError, AttributeError):
                        result.images = []

                # Extract metadata
                if self.config.include_metadata:
                    domain = self.get_domain_from_url(url)
                    content_type = self.detect_content_type(url, result.content, result.metadata)

                    result.metadata = {
                        'title': getattr(crawl_result, 'title', ''),
                        'description': getattr(crawl_result, 'description', ''),
                        'language': getattr(crawl_result, 'language', ''),
                        'status_code': result.status_code,
                        'url': url,
                        'word_count': len(result.content.split()) if result.content else 0,
                        'domain': domain,
                        'content_type': content_type,
                        'scraped_at': result.timestamp,
                        'extraction_method': 'crawl4ai',
                        'links_count': len(result.links) if result.links else 0,
                        'images_count': len(result.images) if result.images else 0,
                    }

                logger.info(f"Successfully processed {url}")
            else:
                error_msg = getattr(crawl_result, 'error_message', 'Unknown error')
                result.error_message = f"Crawl failed: {error_msg}"
                logger.warning(f"Failed to process {url}: {error_msg}")

        except Exception as e:
            result.error_message = f"Result processing error: {str(e)}"
            logger.error(f"Error processing result for {url}: {e}")

        return result

    def create_json_extraction_strategy(self, schema: Dict[str, Any]) -> Any:
        """Create a JSON CSS extraction strategy.

        Args:
            schema: JSON schema for extraction

        Returns:
            JsonCssExtractionStrategy instance
        """
        if not JsonCssExtractionStrategy:
            raise ImportError("JsonCssExtractionStrategy not available")

        return JsonCssExtractionStrategy(schema, verbose=self.config.verbose)

    def create_llm_extraction_strategy(
        self,
        instruction: str,
        provider: str = "openai",
        api_token: Optional[str] = None,
        **kwargs
    ) -> Any:
        """Create an LLM extraction strategy.

        Args:
            instruction: Extraction instruction for the LLM
            provider: LLM provider (openai, huggingface, etc.)
            api_token: API token for the provider
            **kwargs: Additional arguments for the strategy (e.g. base_url, model)

        Returns:
            LLMExtractionStrategy instance
        """
        if not LLMExtractionStrategy:
            raise ImportError("LLMExtractionStrategy not available")

        # Create LLMConfig with provider, token, and extra args
        # We pass kwargs (like base_url, model) to LLMConfig
        llm_config = LLMConfig(provider=provider, api_token=api_token, **kwargs)

        return LLMExtractionStrategy(
            llm_config=llm_config,
            instruction=instruction,
            verbose=self.config.verbose
        )

    def format_results(self, results: List[ScrapingResult], format_type: str = "json") -> str:
        """Format scraping results for output.

        Args:
            results: List of ScrapingResult objects
            format_type: Output format ('json', 'csv', 'markdown')

        Returns:
            Formatted string

        Note:
            If any of the results contain e-commerce URLs from supported retailers
            and format_type is 'csv', the format will be automatically forced to 'json'
            to ensure proper structured data handling for e-commerce content.
        """
        # Check if any results contain e-commerce URLs
        has_ecommerce_urls = any(self.is_ecommerce_url(result.url) for result in results)

        # Force JSON format for e-commerce URLs if CSV was requested
        if has_ecommerce_urls and format_type.lower() == "csv":
            print("Warning: E-commerce URLs detected. Forcing JSON output format instead of CSV for proper structured data handling.")
            format_type = "json"

        if format_type.lower() == "json":
            return json.dumps([asdict(result) for result in results], indent=2)

        elif format_type.lower() == "csv":
            import csv
            import io

            if not results:
                return ""

            output = io.StringIO()

            # Write header
            fieldnames = ['url', 'success', 'content_length', 'links_count',
                         'images_count', 'status_code', 'error_message']
            writer = csv.DictWriter(output, fieldnames=fieldnames)
            writer.writeheader()

            # Write rows
            for result in results:
                writer.writerow({
                    'url': result.url,
                    'success': result.success,
                    'content_length': len(result.content) if result.content else 0,
                    'links_count': len(result.links),
                    'images_count': len(result.images),
                    'status_code': result.status_code or '',
                    'error_message': result.error_message or '',
                })

            return output.getvalue()

        elif format_type.lower() == "markdown":
            if not results:
                return "# No Results\n"

            output = ["# Scraping Results\n"]
            output.append(f"Total URLs processed: {len(results)}")
            output.append(f"Successful: {sum(1 for r in results if r.success)}")
            output.append(f"Failed: {sum(1 for r in results if not r.success)}\n")

            for result in results:
                output.append(f"## {result.url}")
                output.append(f"**Status:** {'✅ Success' if result.success else '❌ Failed'}")

                if result.success:
                    if result.metadata.get('title'):
                        output.append(f"**Title:** {result.metadata['title']}")
                    if result.content:
                        output.append(f"**Content Length:** {len(result.content)} characters")
                    if result.links:
                        output.append(f"**Links Found:** {len(result.links)}")
                    if result.images:
                        output.append(f"**Images Found:** {len(result.images)}")
                    if result.markdown:
                        preview = result.markdown[:200] + "..." if len(result.markdown) > 200 else result.markdown
                        output.append(f"**Content Preview:**\n{preview}")
                else:
                    output.append(f"**Error:** {result.error_message}")

                output.append("")  # Empty line between results

            return "\n".join(output)

        else:
            raise ValueError(f"Unsupported format type: {format_type}")

    def get_domain_from_url(self, url: str) -> str:
        """Extract domain name from URL.

        Args:
            url: URL to extract domain from

        Returns:
            Domain name as string
        """
        try:
            parsed = urllib.parse.urlparse(url)
            domain = parsed.netloc.lower()
            # Remove www. prefix if present
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        except Exception as e:
            logger.warning(f"Failed to extract domain from {url}: {e}")
            return "unknown_domain"

    def detect_content_type(self, url: str, content: str = None, metadata: Dict = None) -> str:
        """Detect content type based on URL, content, and metadata.

        Args:
            url: URL that was scraped
            content: Raw content (optional)
            metadata: Additional metadata (optional)

        Returns:
            Detected content type as string
        """
        if metadata is None:
            metadata = {}

        url_lower = url.lower()

        # Check URL patterns first
        if any(pattern in url_lower for pattern in ['/product', '/item', '/shop', '/buy', '/cart']):
            return 'products'
        elif any(pattern in url_lower for pattern in ['/article', '/blog', '/news', '/post', '/story']):
            return 'articles'
        elif any(pattern in url_lower for pattern in ['/doc', '/documentation', '/guide', '/help', '/wiki']):
            return 'documentation'
        elif any(pattern in url_lower for pattern in ['/api', '/endpoint', '/service']):
            return 'api'
        elif any(pattern in url_lower for pattern in ['/forum', '/discussion', '/thread', '/comment']):
            return 'forum'
        elif any(pattern in url_lower for pattern in ['/video', '/watch', '/play', '/stream']):
            return 'video'

        # Check metadata if available
        title = str(metadata.get('title', '')).lower()
        description = str(metadata.get('description', '')).lower()

        if any(keyword in title + description for keyword in ['product', 'buy', 'price', 'shop', 'cart', 'purchase']):
            return 'products'
        elif any(keyword in title + description for keyword in ['article', 'blog', 'news', 'post', 'story', 'published']):
            return 'articles'
        elif any(keyword in title + description for keyword in ['documentation', 'guide', 'help', 'manual', 'tutorial']):
            return 'documentation'

        # Analyze content if available
        if content:
            content_lower = content.lower()
            if any(keyword in content_lower for keyword in ['price', 'cart', 'checkout', 'buy now', 'add to cart']):
                return 'products'
            elif any(keyword in content_lower for keyword in ['article', 'published', 'author', 'posted on']):
                return 'articles'
            elif any(keyword in content_lower for keyword in ['documentation', 'guide', 'tutorial', 'step by step']):
                return 'documentation'

        # Additional heuristics based on common patterns
        if any(site in domain for site in ['amazon', 'ebay', 'shopify', 'woocommerce'] for domain in [self.get_domain_from_url(url)]):
            return 'products'
        elif any(site in domain for site in ['wikipedia', 'wiki', 'docs'] for domain in [self.get_domain_from_url(url)]):
            return 'documentation'

        # Default fallback
        return 'general'

    def enhance_result_for_organization(self, result: ScrapingResult) -> ScrapingResult:
        """Enhance a scraping result with organization metadata.

        Args:
            result: ScrapingResult to enhance

        Returns:
            Enhanced ScrapingResult
        """
        if not result.metadata:
            result.metadata = {}

        # Add organization-specific metadata
        result.metadata.update({
            'domain': result.metadata.get('domain', self.get_domain_from_url(result.url)),
            'content_type': result.metadata.get('content_type', self.detect_content_type(result.url, result.content, result.metadata)),
            'organization_timestamp': result.timestamp,
            'result_id': f"{result.url}_{int(result.timestamp)}",
            'has_content': bool(result.content and len(result.content.strip()) > 100),
            'has_links': bool(result.links and len(result.links) > 0),
            'has_images': bool(result.images and len(result.images) > 0),
        })

        return result


# Convenience functions for common use cases
async def scrape_single_url(
    url: str,
    config: ScrapingConfig = None,
    extraction_strategy: Optional[Any] = None
) -> ScrapingResult:
    """Scrape a single URL with default configuration.

    Args:
        url: URL to scrape
        config: Optional scraping configuration
        extraction_strategy: Optional extraction strategy

    Returns:
        ScrapingResult object
    """
    wrapper = Crawl4AIWrapper(config or ScrapingConfig())
    async with wrapper:
        return await wrapper.scrape_url(url, extraction_strategy)


async def scrape_multiple_urls(
    urls: List[str],
    config: ScrapingConfig = None,
    extraction_strategy: Optional[Any] = None
) -> List[ScrapingResult]:
    """Scrape multiple URLs with default configuration.

    Args:
        urls: List of URLs to scrape
        config: Optional scraping configuration
        extraction_strategy: Optional extraction strategy

    Returns:
        List of ScrapingResult objects
    """
    wrapper = Crawl4AIWrapper(config or ScrapingConfig())
    async with wrapper:
        return await wrapper.scrape_urls(urls, extraction_strategy)


def create_simple_config(**kwargs) -> ScrapingConfig:
    """Create a ScrapingConfig with common customizations.

    Args:
        **kwargs: Configuration parameters to override

    Returns:
        ScrapingConfig instance
    """
    defaults = {
        'max_concurrent': 3,
        'delay_between_requests': 1.0,
        'timeout': 30,
        'verbose': False,
    }
    defaults.update(kwargs)
    return ScrapingConfig(**defaults)